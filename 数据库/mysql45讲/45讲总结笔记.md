

>   45讲核心点：
>   ------------
>
>   ***事务 :***03, **08，20**
>
>   **日志:**
>
>   **主从(分布式):**
>
>   **锁 :**06,07
>
>   **索引:**04,05,09,10,18,

### 第1&2讲：查询语句和更新语句都是怎么执行的

##### 查询语句：

mysql分为**server和存储引擎**，分别有**连接器，缓存，分析器，优化器，执行器**，分别经过这些地方，

再分析每个位置各自做的事情连接器(用户权限认证,管理连接)，查询缓存（弊大于利，已删除，通常使用外部缓存）,分析器（词法分析，语法分析，对sql做解析，查看语法有没有错误），优化器（使用什么索引&join表的连接顺序）,执行器（先判断有没有执行权限，一行一行的去判断（有索引则去满足索引条件的第一行，没有则去数据库第一行））

**分析器做的事情：**先做词法分析后做语法分析,词法分析主要做的是根据mysql的关键字进行验证和解析，而语法分析会在词法解析的基础上进一步做表名和字段名称的验证和解析；

##### 更新语句

更新语句会执行查询语句的每一个步骤，并且在更新之后，还涉及到两个重要的日志模块：redo log（重做日志）和 binlog（归档日志）。

***Redo Log:***

​	在更新的时候会先更新redo log和内存，在适当的时候进行刷盘

***Bin Log:***

​	binlog是归档日志

***两阶段提交：***第一阶段为prepare 第二阶段为commit。

>   1.  介绍Redo log和bin log，他们之间的区别
>   2.  两阶段提交是什么，不使用可不可以

### 第3讲：事务隔离

***事务四大特性：***ACID

***事务执行可能出现的问题：***脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）

***事务隔离级别：***读未提交（read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（serializable ）

***事务启动方式：***

1.  显式启动事务语句， begin 或 start transaction。配套的提交语句是 commit，回滚语句是 rollback。 

2.  set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。（尽量全设计为set autocommit=1）

    >   长事务的危害：会导致数据库中存在过早的事务视图，不能被删除，大量占用存储空间。
    >
    >   如何避免长事务对业务的影响？：

### 第4讲：索引（上）

***索引的实现方式：***

1.  hash表：哈希表这种结构适用于只有等值查询的场景，没有范围查询
2.  数组：有序数组索引只适用于静态存储引擎，也就是不会改变的数据
3.  N叉搜索树：最常用

**索引类型：**主键索引（聚簇）和非主键索引（需要回表）

### 第5讲：索引（下）

mysql中分为：普通索引，唯一索引，联合索引

**覆盖索引：**如果直接在非主键索引上查到需要的字段，则不需要回表

**最左前缀原则：**

**索引下推**：可以在索引遍历过程中，对**索引中包含的字段先做判断**，直接过滤掉不满足条件的记录，减少回表次数。

### 06 &07 | ：全局锁|表锁|行锁





### 第8讲： 事务到底是隔离的还是不隔离的？

##### 事务的启动时机

 **begin/start transaction** 命令当执行到它们之后的第一个操作 InnoDB 表的语句，事务才真正启动。这时**一致性视图是在第执行第一个快照读语句时创建的；**

**start transaction with consistent snapshot** 马上启动一个事务，立即创建一个一致性视图

##### “快照”在 MVCC 里是怎么工作的？

在可重复读隔离级别下，事务在启动的时候就“拍了个快照”。注意，这个快照是基于整库的。但是并不是把所有数据给拷贝出来，而是生成了一个唯一的事务 ID，叫作 transaction id，并且**顺序严格递增**的



<img src="assets/事务更新状态.png" alt="事务更新状态" style="zoom: 50%;" />

**快照读：**read-view是一个数据结构里面存储了{m_ids:当前活跃事务编号集合；min_trx_id:最小活跃事务id;max_trx_id预分配id，也就是当前最大事务编号+1；creator_trx_id:read_view创建者事务编号}

**read-view版本访问规则：**1.判断是否为当前事务访问，是则可以，2.判断trx_id<min_trx_id，说明事务已经提交，可以访问3.判断trx_id>max_trx_id说明事务是在read-view之后生成的，不能访问 4.判断min_trx_id<trx_id<max_trx_id,成立则去m_ids中寻找，**找到则不能访问**

 InnoDB 为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务 ID。“活跃”指的就是，启动了但还没提交。

总的来说在可重复读下，事务提交有三种情况：

1.  版本未提交，不可见；
2.  版本已提交，但是是在视图创建后提交的，不可见；
3.  版本已提交，而且是在视图创建前提交的，可见。

但是有一种情况比较特殊：**当前读：**updata语句， lock in share mode 或 for update，也都可以读到当前最新的数据

所以在当前读发生时，所有的操作都是基于当前版本已经提交的最新数据来查询或者更新的

##### undoLog版本链

<img src="assets/image-20220224160519085.png" alt="image-20220224160519085" style="zoom:50%;" />

UNDO_LOG版本链不会立即删除，而是mysql确定它一定不会被引用

##### **事务的可重复读的能力是怎么实现的？**

可重复读的核心就是**一致性读**（consistent read）；而事务更新数据的时候，只能用当前读。**如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。**

而读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是：

*   在可重复读隔离级别下，只需要在事务开始的时候**创建一致性视图**，之后事务里的其他查询都共用这个一致性视图；
*   在读提交隔离级别下，**每一个语句执行前都会重新算出一个新的视图**。

##### 事务是如何实现的MVCC呢?

MVCC是基于数据版本对并发事务进行访问

(1)每个事务都有一个事务ID,叫做transaction id(严格递增)
		(2)事务在启动时,找到已提交的最大事务ID记为up_limit_id。
		(3)事务在更新一条语句时,比如id=1改为了id=2.会把id=1和该行之前的row trx_id写到undo log里,
并且在数据页上把id的值改为2,并且把修改这条语句的transaction id记在该行行头

注意：

*   **在快照读情况下，MySQL通过mvcc来避免幻读。**
*   **在当前读情况下，MySQL通过next-key来避免幻读**

##### 为什么rr能实现可重复读而rc不能,分两种情况

(1)快照读的情况下,rr不能更新事务内的up_limit_id,而rc每次会把up_limit_id更新为快照读之前最新已提交事务的transaction id,则rc不能可重复读
		(2)当前读的情况下,rr是利用record lock+gap lock来实现的,而rc没有gap,所以rc不能可重复读

### 第9讲：普通索引和唯一索引，应该怎么选择

##### 索引查询（性能差距不大）

**普通索引：**需要查找下一个记录，直到碰到第一个不满足条件的记录。

**唯一索引：**查找到第一个满足条件的记录后，就会停止

##### 更新过程

当更新一个数据页时，如果数据页在内存中就直接更新，如果数据页还没有在内存中，InooDB 会将这些更新操作**缓存在 change buffer** 中，在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge。除了访问这个数据页会触发 merge 外，系统有后台线程会定期 merge。在数据库正常关闭（shutdown）的过程中，也会执行 merge 操作。

而对于唯一索引来说：每次插入都需要判断是否违反**唯一性约束**，所以需要将数据页读入内存中，既然这样对于唯一索引的更新就可以直接在数据页上更新，不需要change buffer 。而普通索引不需要唯一性判断，所以可以使用change buffer 去更新，所以尽量使用普通索引可以提高性能

##### change buffer是写内存的，如果机器掉电重启，会不会导致 change buffer 丢失

会导致change buffer丢失，会导致本次未完成的操作数据丢失，但不会导致已完成操作的数据丢失。
		change buffer中分两部分，一部分是本次写入未写完的，一部分是已经写入完成的。
			1.针对未写完的，此部分操作，还未写入redo log，因此事务还未提交，所以没影响。
			2.针对，已经写完成的，可以通过redo log来进行恢复

### 第10讲:10 MySQL为什么有时候会选错索引？

##### 慢查询日志

long_query_time：超过这个值的sql语句会被记录在日志中，默认为10；可以看到具体语句，再使用分析工具分析

日志分析工具mysqldumpslow

>   mysqldumpslow是官方提供的慢查询日志分析工具。主要功能包括统计不同慢 sql 的
>
>   *   出现次数(Count)
>   *   执行耗费的平均时间和累计总耗费时间(Time)
>   *   等待锁耗费的时间(Lock)
>   *   发送给客户端的行总数(Rows)
>   *   扫描的行总数(Rows)
>   *   用户以及 SQL 语句本身(抽象了一下格式，比如 limit 1, 20 用 limit N,N 表示)

##### 优化器选择索引

优化器会根据扫描行数，是否回表，是否使用索引，临时表等信息来判断使用哪个索引效率更高。

**扫描行数是怎么判断的？**一个索引上不同的值越多，这个索引的区分度就越好。而一个索引上不同的值的个数，我们称之为“基数”（cardinality）。也就是说，这个基数越大，索引的区分度越好。通过**show index的cardinality可以查看区分度**，但是这个区分度其实是通过**取样**来得到的，所以结果可能不准确，而选错索引的罪魁祸首也是它



##### 索引选择异常和处理

有时候Mysql会选错索引，导致Sql语句很慢，所以我们可以考虑使用**force index 强行选择一个索引**，来矫正mysql的选择，如果不想这样做（因为需要更改sql语句，并且索引变化，还需要更改sql）,那我们可以修改我们的语句，让其自动选择我们期望的索引，第三种方法我们可以**新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引。**



### 11 | 怎么给字符串字段加索引？

1.   前缀索引：但是需要定义好长度，增加区分度，否则可能会增加很多扫描的次数；还有可能对覆盖索引有影响。
2.   **倒序存储**：不支持范围查询,只支持等值查询，会增加扫描次数
3.   **hash 字段**: 不支持范围查询，支持等值查询，查询比较快，稳定

### 12讲：Mysql突然变慢的原因分析

##### 刷脏页

1.   Redo log满：redos log 满了之后，innodb需要将数据刷入磁盘

2.   内存不足：内存不足的时候需要淘汰页面，这个页面可能是脏页所以需要将脏页进行刷盘

3.   Mysql认为空闲的时候
4.   Mysql关闭的时候

##### InnoDB 刷脏页的控制策略

所以为了防止这种情况我们需要有控制脏页比例的机制，来尽量避免上面的这两种情况

*   怎么去确定自己innodb_io_capacity的大小，通过脏页比例和redo log写入速度来决定。
*   innodb_flush_neighbors 参数就是用来控制刷脏页时相邻脏页也刷，innodb_flush_neighbors 值为 1 的时候会有上述的“连坐”机制，值为 0 时表示不找邻居，自己刷自己的。

### 16 | “order by”是怎么工作的？

##### 全字段排序

在我们执行`select city,name,age from t where city='杭州' order by name limit 1000  ;`语句时，mysql会进行全字段排序，也就是将city=杭州的从索引中查找出来，放到内存中，进行快排，然后去前100个结果，不过可能由于数据太大，品牌需也有可能需要使用外部排序，这取决于排序所需的内存和参数sort_buffer_size，我们可以通过打开`SET optimizer_trace='enabled=on'; `查看 OPTIMIZER_TRACE 的结果来确认的，你可以从 number_of_tmp_files 中看到是否使用了临时文件。

<img src="assets/OPTIMIZER_TRACE 部分结果.png" alt="OPTIMIZER_TRACE 部分结果" style="zoom:67%;" />

number_of_tmp_files 表示的是，排序过程中使用的临时文件数,外部排序一般使用归并排序算法。，**将排序的数据分成 12 份，每一份单独排序后存在这些临时文件中。然后把这 12 个有序文件再合并成一个有序的大文件。**

##### rowid 排序

`max_length_for_sort_data`，是 MySQL 中专门控制用于排序的行数据的长度的一个参数。如果单行的长度超过这个值，MySQL 就认为单行太大，要换一个算法,单行太大，加载到内存的就是需要排序的字段+id，排序完后就可以通过主键索引id去查找剩余的字段，所以**rowid 排序多访问了一次表 t 的主键索引**

所以对比<u>rowid 排序</u>和<u>全字段排序</u>如果内存够mysql就会优先使用内存，提高效率，但是内存不够就只能多访问一次磁盘。

##### order by不用排序的情况

当mysql取出的数据天然有序的话，就不会再使用排序算法排序，而是直接将访问的记录添加到结果集，**使用了覆盖索引**

### 18 | 哪些情况可能导致索引失效？

##### 案例一：对索引字段进行函数操作

注意，对字段进行函数计算，<u>并不是会使数据库完全放弃使用这个索引</u>，还可能使用这个索引，但是<u>可能会全索引扫描</u>

注意：索引字段不能进行**函数操作**，但是索引字段的参数可以玩函数（其参数可以使用函数）

##### 案例二：隐式类型转换

##### 案例三：隐式字符编码转换

两个比较，字符集不同，那么可能会造成字符隐式转换

### 19 | 为什么我只查一行的语句，也执行这么慢

##### 第一类：长时间不返回

1.等 MDL 锁

大概率是表 t 被锁住了。一般都是首先执行一下 <u>show processlist 命令</u>，看看当前语句处于什么状态，state字段可以显示当前语句的状态，可以找到持有MDL锁的语句kill掉

2.等 flush

有一个线程正要对表 t 做 flush 操作，不过正常情况下flush操作也很快，非常有可能flush操作被别的线程堵住了

3.等行锁

##### 第二类：查询慢

带 lock in share mode 的 SQL 语句，是**当前读**，所以速度很快；而 select * from t where id=1 这个语句，是一致性读，因此需要从 最新的结果开始开始，依次执行 undo log，执行了 100 万次以后，才将 一致性读的 结果返回。



### 20 | 幻读是什么，幻读有什么问题？

**脏读:** 对于两个事物 T1, T2, T1 读取了已经被 T2 更新但还没有被提交的字段. 之后, 若 T2 回滚, T1读取的内容就是临时且无效的. 

**不可重复读:** 对于两个事物 T1, T2, T1 读取了一个字段, 然后 T2 更新了该字段. 之后, T1再次读取同一个字段, 值就不同了. 

**幻读:** 对于两个事物 T1, T2, T1 从一个表中读取了一个字段, 然后 T2 在该表中插入了一些新的行. 之后, 如果 T1 再次读取同一个表, 就会多出几行. 

|                         事务隔离级别                         | 可能出现的问题         | 解决办法 |
| :----------------------------------------------------------: | ---------------------- | -------- |
|                 读未提交（read uncommitted）                 | 脏读、不可重复读和幻读 |          |
|        读提交（read committed）可以读到已经提交的数据        | 不可重复读和幻读问题   |          |
| 可重复读（repeatable read）//除非当前读，否则不能读到已提交的数据 | 幻读                   |          |
|                   串行化（serializable ）                    | 所有问题都可以避免     |          |

##### 为什么可重复读可能发生幻读

首先：只有<u>当前读</u>模式下，才有可能发生幻读问题，幻读问题是因为出现了之前查询没有出现的行，如果是对已有的行进行操作，会加锁，但是对于没有出现的行，则没有办法加锁，所以才会出现两次的查询结果不同

所以如何解决这个问题呢？引入了间隙锁（Gap Lock），间隙锁和行锁合称 next-key lock，每个 next-key lock 是**前开后闭区间**，**Gap Lock为开区间**，虽然Gap Lock的引入解决了幻读问题，但是又引入了新的问题--**死锁**，如果不想使用间隙锁，可以考虑将隔离级别设为读提交，这样就没有间隙锁了，但需要解决可能出现的数据和日志不一致问题，需要把 binlog 格式设置为 row，这也是现在不少公司使用的配置组合。

##### 半一致性读

这是一种夹在**普通读和锁定读**之间的一种读取方式。它只在<u>`READ COMMITTED`隔离级别下使用`UPDATE`语句</u>时才会使用。具体的含义就是当`UPDATE`语句读取已经被其他事务加了锁的记录时，`InnoDB`会将该记录的最新提交的版本读出来，然后判断该版本是否与`UPDATE`语句中的`WHERE`条件相匹配，如果不匹配则不对该记录加锁，从而跳到下一条记录；如果匹配则再次读取该记录并对其进行加锁。这样子处理只是为了让`UPDATE`语句尽量少被别的语句阻塞。


作者：小孩子4919
链接：https://juejin.cn/post/6844904022499917838
来源：稀土掘金
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

### 21 | 深入理解间隙锁的加锁原则？

**我总结的加锁规则里面，包含了两个“原则”、两个“优化”和一个“bug”。**

1.  原则 1：加锁的基本单位是 next-key lock。希望你还记得，next-key lock 是前开后闭区间。
2.  原则 2：查找过程中访问到的对象才会加锁。
3.  优化 1：索引上的等值查询，给唯一索引加锁的时候，next-key lock 退化为行锁。
4.  优化 2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁。
5.  一个 bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。

### 23 | MySQL是怎么保证数据不丢的？

##### page cache和Buffer cache

##### binlog 的写入机制

事务执行过程中，先把日志写到 binlog cache，事务提交的时候，再把 binlog cache 写到 binlog 文件（先到文件系统的page cache）中。

一个事务的 **binlog 是不能被拆开的**，因此不论这个事务多大，也要确保一次性写入。这就涉及到了 binlog cache 的保存问题。

<img src="assets/binlog写盘.png" alt="binlog写盘" style="zoom:43%;" />

每个线程有自己 binlog cache，但是共用同一份 binlog 文件。

*   图中的 write，指的就是指把日志写入到<u>文件系统的 page cache</u>，并没有把数据持久化到磁盘，所以速度比较快。
*   图中的 fsync，才是将数据持久化到磁盘的操作。一般情况下，我们认为 f<u>sync 才占磁盘的 IOPS</u>。

write 和 fsync 的时机，是由参数 sync_binlog 控制的：

1.  sync_binlog=0 的时候，表示每次提交事务都只 write，不 fsync；
2.  sync_binlog=1 的时候，表示每次提交事务都会执行 fsync；
3.  sync_binlog=N(N>1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync。

##### redo log 的写入机制

 <u>redo log buffer和change buffer？？</u>

事务在执行过程中，生成的 redo log 是要先写到 redo log buffer 的。然后在wirte到文件系统的page cache上 ，在持久化到磁盘上

为了控制 redo log 的写入策略，InnoDB 提供了 innodb_flush_log_at_trx_commit 参数，它有三种可能取值：

1.  设置为 0 的时候，表示每次事务提交时都只是把 redo log 留在 redo log buffer 中 ;
2.  设置为 1 的时候，表示每次事务提交时都将 redo log 直接持久化到磁盘；
3.  设置为 2 的时候，表示每次事务提交时都只是把 redo log 写到 page cache。



***什么情况下会出现没有提交的事务写入到磁盘？***

1.   InnoDB 有一个后台线程，每隔 1 秒，就会把 **redo log buffer 中的日志**，调用 write 写到文件系统的 page cache，然后调用 fsync 持久化到磁盘。

注意，事务执行中间过程的 redo log 也是直接写在 redo log buffer 中的，这些 redo log 也会被后台线程一起持久化到磁盘。也就是说，一个没有提交的事务的 redo log，也是可能已经持久化到磁盘的。

2.   **redo log buffer 占用的空间即将达到 innodb_log_buffer_size 一半的时候，后台线程会主动写盘**
3.   **并行的事务提交的时候，顺带将这个事务的 redo log buffer 持久化到磁盘**

##### 组提交

TODO(没懂)

### 24 | MySQL是怎么保证主备一致的？

一个事务日志同步的完整过程是这样的：

1.  在备库 B 上通过 change master 命令，设置主库 A 的 IP、端口、用户名、密码，以及要从哪个位置开始请求 binlog，这个位置包含文件名和日志偏移量。
2.  在备库 B 上执行 start slave 命令，这时候备库会启动两个线程，就是图中的 io_thread 和 sql_thread。其中 io_thread 负责与主库建立连接。
3.  主库 A 校验完用户名、密码后，开始按照备库 B 传过来的位置，从本地读取 binlog，发给 B。
4.  备库 B 拿到 binlog 后，写到本地文件，称为中转日志（relay log）。
5.  sql_thread 读取中转日志，解析出日志里的命令，并执行。

##### binLog的三种格式

statement：记录的SQL语句的原文

row：binlog 里面记录了真实删除行的主键 id，优点是安全，缺点是很占空间。比如你用一个 delete 语句删掉 10 万行数据，用 statement 的话就是一个 SQL 语句被记录到 binlog 中，占用几十个字节的空间。但如果用 row 格式的 binlog，就要把这 10 万条记录都写到 binlog 中

mixed：两种方式的混合，

##### 对于三种SQL语句delete、insert 和 update：

​	执行的是 delete 语句，row 格式的 binlog 也会把被删掉的行的整行信息保存起来。所以，如果你在执行完一条 delete 语句以后，发现删错数据了，可以直接把 binlog 中记录的 delete 语句转成 insert，把被错删的数据插入回去就可以恢复了。

如果你是执行错了insert 语句呢？那就更直接了。row 格式下，insert 语句的 binlog 里会记录所有的字段信息，这些信息可以用来精确定位刚刚被插入的那一行。这时，你直接把 insert 语句转成 delete 语句，删除掉这被误插入的一行数据就可以了。

如果执行的是 update 语句的话，binlog 里面会记录修改前整行的数据和修改后的整行数据。所以，如果你误执行了 update 语句的话，只需要把这个 event 前后的两行信息对调一下，再去数据库里面执行，就能恢复这个更新操作了

##### 循环复制

双M结构，如果两个服务器互为备库，那么在A服务器上生成的binlog会发给B服务器，B服务器在生成binlog，发给A就可能造成循环复制的问题，所以如何解决呢？

MySQL 在 binlog 中记录了**这个命令第一次执行时所在实例的 server id**。因此，我们可以用下面的逻辑，来解决两个节点间的循环复制的问题：

1.  规定两个库的 server id 必须不同，如果相同，则它们之间不能设定为主备关系；
2.  一个备库接到 binlog 并在重放的过程中，**生成与原 binlog 的 server id 相同的新的 binlog；**
3.  每个库在收到从自己的主库发过来的日志后，先判断 server id，如果跟自己的相同，表示这个日志是自己生成的，就直接丢弃这个日志。

### 25 | MySQL是怎么保证高可用的？

seconds_behind_master，用于表示当前备库延迟了多少秒。

##### 主备延迟

1.   备库机器性能比主库差
2.   备库上除了处理主库过来的同步日志，还做了其他的一些工作，导致多个任务都在争抢资源，备库来不及处理同步日志，导致主备延迟
3.   大事务，如果一个事务在主库上执行10分钟，那么在同步到从库上，就可能导致主备延迟较久，所以不要**一次性地用 delete 语句删除太多数**，可以考虑分组多批次的进行删除

##### 主备延迟如何解决

**可靠性优先策略：**等待备库完成同步，再开启查询，但是主备延迟时间或长或短，如果太长可能导致系统长时间不可用

**可用性优先策略：**将binlog设置为row格式，数据不一致问题更容易被发现，如果为statement，则可能悄无声息的导致数据不可用问题

### 45讲：自增id用完怎么办？

##### 1.表定义自增值 id

对于自增id，如果达到上限了之后，再执行插入语句就会报主键冲突，所以如果表的数据比较大，可以将主键id定义为8 个字节的 bigint unsigned

##### 2.InnoDB 系统自增 row_id

 InnoDB 表没有指定主键，那么 InnoDB 会给你创建一个不可见的，长度为 6 个字节的 row_id。InnoDB 维护了一个全局的 dict_sys.row_id 值，所有无主键的 InnoDB 表，每插入一行数据，都将当前的 dict_sys.row_id 值作为要插入数据的 row_id，然后把 dict_sys.row_id 的值加 1。当这种情况下，写入表的 row_id 是从 0 开始到 2^48-1,达到上限后，下一个值就是 0，然后继续循环。所以此种情况有覆盖数据的风险

##### 3.Xid

##### Innodb trx_id

InnoDB 内部维护了一个 max_trx_id 全局变量，每次需要申请一个新的 trx_id 时，就获得 max_trx_id 的当前值，然后并将 max_trx_id 加 1。

InnoDB 数据可见性的核心思想是：每一行数据都记录了更新它的 trx_id，当一个事务读到一行数据的时候，判断这个数据是否可见的方法，就是**通过事务的一致性视图与这行数据的 trx_id 做对比**。

##### 5.thread_id

show processlist 里面的第一列，就是 thread_id。系统保存了一个全局变量 thread_id_counter，每新建一个连接，就将 thread_id_counter 赋值给这个新连接的线程变量，thread_id_counter 定义的大小是 4 个字节，因此达到 232-1 后，它就会重置为 0，但是不会在 show processlist 里看到两个相同的 thread_id，因为在线程分配时代码会判断其唯一性

